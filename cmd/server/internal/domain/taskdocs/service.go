package taskdocs

import (
	"encoding/json"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"strings"
	"time"
)

// appendChunkInternalæ‰§è¡Œè¿½åŠ é€»è¾‘(å†…éƒ¨å®ç°)
// è¿”å› duplicate=true è¡¨ç¤ºæœªå†™å…¥
func appendChunkInternal(projectID, taskID, docType, content, op, user, source string, expectedVersion *int) (meta DocMeta, newChunk *DocChunk, duplicate bool, err error) {
	start := time.Now()

	meta, err = loadOrInitMeta(projectID, taskID, docType)
	if err != nil {
		return
	}

	if expectedVersion != nil && meta.Version != *expectedVersion {
		err = fmt.Errorf("version_mismatch")
		return
	}

	h := hashDocContent(content)

	// é‡å¤æ£€æµ‹: è‹¥å‘½ä¸­å“ˆå¸Œçª—å£åˆ™ç›´æ¥è¿”å›
	if containsHash(meta.HashWindow, h) {
		compiledPath, _ := docCompiledPath(projectID, taskID, docType)
		var compiledSize int64
		if fi, statErr := os.Stat(compiledPath); statErr == nil {
			compiledSize = fi.Size()
		}
		log.Printf("[DOC_APPEND] pid=%s tid=%s doc=%s seq=%d ver=%d op=%s duplicate=true content_size=%d total_chunks=%d compiled_size=%d dur_ms=%d",
			projectID, taskID, docType, meta.LastSequence, meta.Version, op, len(content), meta.ChunkCount, compiledSize, time.Since(start).Milliseconds())
		return meta, nil, true, nil
	}

	if op == "" {
		op = "add_full"
	}
	if source == "" {
		source = "api"
	}

	seq := meta.LastSequence + 1
	ck := DocChunk{
		Sequence:  seq,
		Timestamp: time.Now(),
		Op:        op,
		Content:   content,
		User:      user,
		Source:    source,
		Hash:      h,
		Active:    true,
	}

	// å†™å…¥ chunk
	cp, err := docChunksPath(projectID, taskID, docType)
	if err != nil {
		return
	}

	if err = os.MkdirAll(filepath.Dir(cp), 0755); err != nil {
		return
	}

	f, err := os.OpenFile(cp, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644)
	if err != nil {
		return
	}

	enc, _ := json.Marshal(ck)
	if _, err = f.Write(append(enc, '\n')); err != nil {
		f.Close()
		return
	}
	f.Close()

	// æ›´æ–° compiled
	compiledPath, _ := docCompiledPath(projectID, taskID, docType)
	needSectionSync := false
	if op == "replace_full" {
		_ = os.WriteFile(compiledPath, []byte(content), 0644)
		needSectionSync = true // å…¨æ–‡æ›¿æ¢åéœ€è¦é‡æ–°è§£æç« èŠ‚
	} else if strings.HasPrefix(op, "section_") && strings.HasSuffix(op, "_no_parse") {
		// ğŸ”§ ç‰¹æ®Šæ“ä½œï¼šç« èŠ‚çº§æ“ä½œå·²ç»åœ¨ SectionService ä¸­å¤„ç†äº†åŒæ­¥ï¼Œä¸éœ€è¦å†æ¬¡è§£æ
		_ = os.WriteFile(compiledPath, []byte(content), 0644)
		needSectionSync = false
	} else {
		// append æ¨¡å¼
		if _, statErr := os.Stat(compiledPath); statErr == nil {
			rf, _ := os.OpenFile(compiledPath, os.O_WRONLY|os.O_APPEND, 0644)
			if rf != nil {
				_, _ = rf.Write([]byte("\n" + content))
				rf.Close()
			}
		} else {
			_ = os.WriteFile(compiledPath, []byte(content), 0644)
		}
	}

	compiledBytes, _ := os.ReadFile(compiledPath)

	// æ›´æ–° meta
	meta.Version++
	meta.LastSequence = seq
	meta.HashWindow = pushHashWindow(meta.HashWindow, h)
	meta.ChunkCount++
	meta.UpdatedAt = time.Now()
	meta.ETag = hashDocContent(string(compiledBytes))

	if err = writeMetaAtomic(projectID, taskID, docType, meta); err != nil {
		return
	}

	// å¦‚æœæ˜¯ replace_full æ“ä½œï¼Œé‡æ–°è§£æç« èŠ‚ç»“æ„
	if needSectionSync {
		docPath, pathErr := docBaseDir(projectID, taskID, docType)
		if pathErr == nil {
			sm := NewSyncManager(docPath, docType)
			log.Printf("[DOC_APPEND] pid=%s tid=%s doc=%s section_sync_start compiled_size=%d", projectID, taskID, docType, len(compiledBytes))
			if syncErr := sm.SyncFromCompiled(); syncErr != nil {
				log.Printf("[DOC_APPEND] pid=%s tid=%s doc=%s section_sync_error=%v", projectID, taskID, docType, syncErr)
			} else {
				// é‡æ–°è¯»å– compiled.md ç¡®è®¤å†…å®¹
				verifyBytes, _ := os.ReadFile(compiledPath)
				log.Printf("[DOC_APPEND] pid=%s tid=%s doc=%s section_sync=success compiled_size_after=%d", projectID, taskID, docType, len(verifyBytes))
			}
		}
	}

	log.Printf("[DOC_APPEND] pid=%s tid=%s doc=%s seq=%d ver=%d op=%s duplicate=false content_size=%d total_chunks=%d compiled_size=%d dur_ms=%d",
		projectID, taskID, docType, seq, meta.Version, op, len(content), meta.ChunkCount, len(compiledBytes), time.Since(start).Milliseconds())

	return meta, &ck, false, nil
}

// rebuildCompiled å…¨é‡é‡å»º compiled ä¸ meta
func rebuildCompiled(projectID, taskID, docType string) (DocMeta, error) {
	start := time.Now()

	chunks, meta, err := listChunks(projectID, taskID, docType)
	if err != nil {
		return meta, err
	}

	var bldr []string
	maxSeq := 0
	activeCount := 0
	deleted := 0
	var lastHashes []string

	for _, ck := range chunks {
		if ck.Sequence > maxSeq {
			maxSeq = ck.Sequence
		}

		if ck.Active {
			activeCount++
			if ck.Content != "" {
				// å¦‚æœæ˜¯ replace_full æˆ– section_full_no_parseï¼Œæ¸…ç©ºä¹‹å‰çš„å†…å®¹ï¼Œä»è¿™ä¸ª chunk å¼€å§‹é‡æ–°æ„å»º
				if ck.Op == "replace_full" || ck.Op == "section_full_no_parse" {
					bldr = []string{ck.Content}
				} else {
					// å…¶ä»–æ“ä½œç±»å‹ï¼ˆadd_full ç­‰ï¼‰éƒ½æ˜¯è¿½åŠ 
					bldr = append(bldr, ck.Content)
				}
			}
			lastHashes = pushHashWindow(lastHashes, ck.Hash)
		} else {
			deleted++
		}
	}

	compiled := strings.Join(bldr, "\n")

	cp, _ := docCompiledPath(projectID, taskID, docType)
	_ = os.WriteFile(cp, []byte(compiled), 0644)

	meta.LastSequence = maxSeq
	meta.ChunkCount = len(chunks)
	meta.DeletedCount = deleted
	meta.HashWindow = lastHashes
	meta.UpdatedAt = time.Now()
	meta.ETag = hashDocContent(compiled)

	if err = writeMetaAtomic(projectID, taskID, docType, meta); err != nil {
		return meta, err
	}

	log.Printf("[DOC_REBUILD] pid=%s tid=%s doc=%s total_chunks=%d active_chunks=%d deleted_chunks=%d compiled_size=%d dur_ms=%d",
		projectID, taskID, docType, len(chunks), activeCount, deleted, len(compiled), time.Since(start).Milliseconds())

	return meta, nil
}

// logicalDeleteChunk è½¯åˆ é™¤ chunk
func logicalDeleteChunk(projectID, taskID, docType string, seq int) (DocMeta, error) {
	start := time.Now()

	cp, err := docChunksPath(projectID, taskID, docType)
	if err != nil {
		return DocMeta{}, err
	}

	data, err := os.ReadFile(cp)
	if err != nil {
		return DocMeta{}, err
	}

	lines := strings.Split(string(data), "\n")
	changed := false

	for i, ln := range lines {
		if strings.TrimSpace(ln) == "" {
			continue
		}

		var ck DocChunk
		if json.Unmarshal([]byte(ln), &ck) == nil && ck.Sequence == seq && ck.Active {
			ck.Active = false
			enc, _ := json.Marshal(ck)
			lines[i] = string(enc)
			changed = true
			break
		}
	}

	meta, _ := loadOrInitMeta(projectID, taskID, docType)

	if !changed {
		log.Printf("[DOC_DELETE] pid=%s tid=%s doc=%s seq=%d found=false ver=%d dur_ms=%d",
			projectID, taskID, docType, seq, meta.Version, time.Since(start).Milliseconds())
		return meta, nil
	}

	if err := os.WriteFile(cp, []byte(strings.Join(lines, "\n")), 0644); err != nil {
		return DocMeta{}, err
	}

	meta, err = rebuildCompiled(projectID, taskID, docType)
	if err != nil {
		return meta, err
	}

	compiledPath, _ := docCompiledPath(projectID, taskID, docType)
	var compiledSize int64
	if fi, statErr := os.Stat(compiledPath); statErr == nil {
		compiledSize = fi.Size()
	}

	log.Printf("[DOC_DELETE] pid=%s tid=%s doc=%s seq=%d found=true ver=%d total_chunks=%d deleted_chunks=%d compiled_size=%d dur_ms=%d",
		projectID, taskID, docType, seq, meta.Version, meta.ChunkCount, meta.DeletedCount, compiledSize, time.Since(start).Milliseconds())

	return meta, nil
}

// Append å°è£…åº•å±‚ appendChunkInternalï¼Œæä¾›å¹¶å‘äº’æ–¥
func (s *DocService) Append(projectID, taskID, docType, content, user string, expectedVersion *int, op, source string) (DocMeta, *DocChunk, bool, error) {
	// åªåœ¨é replace_full æ¨¡å¼ä¸‹æ‹’ç»ç©ºå†…å®¹
	// replace_full å…è®¸ç©ºå†…å®¹ï¼ˆç”¨æˆ·å¯èƒ½æƒ³æ¸…ç©ºæ–‡æ¡£ï¼‰
	if strings.TrimSpace(content) == "" && op != "replace_full" {
		return DocMeta{}, nil, false, fmt.Errorf("empty_content")
	}

	l := s.GetLock(projectID, taskID, docType)
	l.Lock()
	defer l.Unlock()

	return appendChunkInternal(projectID, taskID, docType, content, op, user, source, expectedVersion)
}

// List è¿”å› chunks + meta
func (s *DocService) List(projectID, taskID, docType string) ([]DocChunk, DocMeta, error) {
	l := s.GetLock(projectID, taskID, docType)
	l.Lock()
	defer l.Unlock()

	return listChunks(projectID, taskID, docType)
}

// Delete é€»è¾‘åˆ é™¤å¹¶é‡å»º
func (s *DocService) Delete(projectID, taskID, docType string, seq int) (DocMeta, error) {
	l := s.GetLock(projectID, taskID, docType)
	l.Lock()
	defer l.Unlock()

	return logicalDeleteChunk(projectID, taskID, docType, seq)
}

// Rebuild å¤–éƒ¨è§¦å‘é‡å»º
func (s *DocService) Rebuild(projectID, taskID, docType string) (DocMeta, error) {
	l := s.GetLock(projectID, taskID, docType)
	l.Lock()
	defer l.Unlock()

	return rebuildCompiled(projectID, taskID, docType)
}

// Toggle æ¿€æ´»/åœç”¨æŸä¸ª chunk
func (s *DocService) Toggle(projectID, taskID, docType string, seq int) (DocMeta, error) {
	l := s.GetLock(projectID, taskID, docType)
	l.Lock()
	defer l.Unlock()

	start := time.Now()

	cp, err := docChunksPath(projectID, taskID, docType)
	if err != nil {
		return DocMeta{}, err
	}

	data, err := os.ReadFile(cp)
	if err != nil {
		return DocMeta{}, err
	}

	lines := strings.Split(string(data), "\n")
	changed := false

	for i, ln := range lines {
		if strings.TrimSpace(ln) == "" {
			continue
		}

		var ck DocChunk
		if json.Unmarshal([]byte(ln), &ck) == nil && ck.Sequence == seq {
			ck.Active = !ck.Active
			enc, _ := json.Marshal(ck)
			lines[i] = string(enc)
			changed = true
			break
		}
	}

	meta, _ := loadOrInitMeta(projectID, taskID, docType)

	if !changed {
		log.Printf("[DOC_TOGGLE] pid=%s tid=%s doc=%s seq=%d found=false ver=%d dur_ms=%d",
			projectID, taskID, docType, seq, meta.Version, time.Since(start).Milliseconds())
		return meta, nil
	}

	if err := os.WriteFile(cp, []byte(strings.Join(lines, "\n")), 0644); err != nil {
		return DocMeta{}, err
	}

	meta, err = rebuildCompiled(projectID, taskID, docType)
	if err != nil {
		return meta, err
	}

	compiledPath, _ := docCompiledPath(projectID, taskID, docType)
	var compiledSize int64
	if fi, statErr := os.Stat(compiledPath); statErr == nil {
		compiledSize = fi.Size()
	}

	log.Printf("[DOC_TOGGLE] pid=%s tid=%s doc=%s seq=%d found=true ver=%d total_chunks=%d deleted_chunks=%d compiled_size=%d dur_ms=%d",
		projectID, taskID, docType, seq, meta.Version, meta.ChunkCount, meta.DeletedCount, compiledSize, time.Since(start).Milliseconds())

	return meta, nil
}

// Squash åˆå¹¶æ‰€æœ‰ active chunks ä¸ºå•ä¸€ chunk
func (s *DocService) Squash(projectID, taskID, docType, user string, expectedVersion *int) (DocMeta, error) {
	l := s.GetLock(projectID, taskID, docType)
	l.Lock()
	defer l.Unlock()

	start := time.Now()

	meta, err := loadOrInitMeta(projectID, taskID, docType)
	if err != nil {
		return meta, err
	}

	if expectedVersion != nil && meta.Version != *expectedVersion {
		return meta, fmt.Errorf("version_mismatch")
	}

	// å…ˆ rebuild ç¡®ä¿ä¸€è‡´æ€§
	if _, err := rebuildCompiled(projectID, taskID, docType); err != nil {
		return meta, err
	}

	compiledPath, _ := docCompiledPath(projectID, taskID, docType)
	compiledBytes, err := os.ReadFile(compiledPath)
	if err != nil {
		return meta, err
	}

	merged := string(compiledBytes)

	// å½’æ¡£æ—§æ–‡ä»¶
	cp, _ := docChunksPath(projectID, taskID, docType)
	if _, statErr := os.Stat(cp); statErr == nil {
		backup := cp + ".bak-" + time.Now().Format("20060102T150405")
		_ = os.Rename(cp, backup)
	}

	// åˆ›å»ºæ–°çš„å•ä¸€ chunk
	seq := meta.LastSequence + 1
	ck := DocChunk{
		Sequence:  seq,
		Timestamp: time.Now(),
		Op:        "replace_full",
		Content:   merged,
		User:      user,
		Source:    "squash",
		Hash:      hashDocContent(merged),
		Active:    true,
	}

	enc, _ := json.Marshal(ck)
	if err := os.WriteFile(cp, append(enc, '\n'), 0644); err != nil {
		return meta, err
	}

	if err := os.WriteFile(compiledPath, []byte(merged), 0644); err != nil {
		return meta, err
	}

	meta.Version++
	meta.LastSequence = seq
	meta.ChunkCount = 1
	meta.DeletedCount = 0
	meta.HashWindow = pushHashWindow([]string{}, ck.Hash)
	meta.UpdatedAt = time.Now()
	meta.ETag = hashDocContent(merged)

	if err := writeMetaAtomic(projectID, taskID, docType, meta); err != nil {
		return meta, err
	}

	log.Printf("[DOC_SQUASH] pid=%s tid=%s doc=%s seq=%d ver=%d merged_size=%d dur_ms=%d",
		projectID, taskID, docType, seq, meta.Version, len(merged), time.Since(start).Milliseconds())

	return meta, nil
}
